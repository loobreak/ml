# 梯度下降

## 目标函数

目标函数是一个关于参数的函数，我们的目标是找到这个函数$\theta$的最小值。在线性回归中，我们的目标函数是均方误差函数，即：

$J(\Theta) = \frac{1}{2m}\sum_{i=1} ^m(y^i - h_\theta(x^i))^2$

对目标函数求 $\theta$ 偏导:

$\frac{\partial J(\Theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1} ^m(y^i - h_\theta(x^i))x_j^i$

当存在多个特征时，且特征之间的量纲不同，需要对特征进行归一化处理，使得特征之间的量纲相同，这样可以加快梯度下降的收敛速度。

### 批量梯度下降

批量梯度下降是一种最基本的梯度下降算法，它的思想是：在每一次迭代中，计算所有样本的梯度，然后更新参数。具体的更新公式如下：

$\theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1} ^m(y^i - h_\theta(x^i))x_j^i$

其中，$\alpha$ 是学习率，控制了参数更新的步长；批量梯度下降的缺点是每次迭代都要计算所有样本的梯度，计算开销较大。

### 随机梯度下降

随机梯度下降是一种每次迭代只计算一个样本梯度的算法，具体的更新公式如下：

$\theta_j = \theta_j - \alpha(y^i - h_\theta(x^i))x_j^i$

随机梯度下降的优点是计算开销小，但是由于每次迭代只使用一个样本，参数更新的方向不一定是最优的，可能会导致收敛速度较慢。

### 小批量梯度下降

小批量梯度下降是批量梯度下降和随机梯度下降的折中，每次迭代计算一小部分样本的梯度，具体的更新公式如下：

$\theta_j = \theta_j - \alpha\frac{1}{b}\sum_{i=1} ^b(y^i - h_\theta(x^i))x_j^i$

其中，$b$ 是小批量的大小，通常取 32、64 等值。